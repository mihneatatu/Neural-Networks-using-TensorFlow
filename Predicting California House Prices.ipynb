{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a217036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f281a",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d22f53",
   "metadata": {},
   "source": [
    "Import the cal_housing_clean.csv file with pandas. Separate it into a training (70%) and testing set(30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31345968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>housingMedianAge</th>\n",
       "      <th>totalRooms</th>\n",
       "      <th>totalBedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>medianIncome</th>\n",
       "      <th>medianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   housingMedianAge  totalRooms  totalBedrooms  population  households  \\\n",
       "0              41.0       880.0          129.0       322.0       126.0   \n",
       "1              21.0      7099.0         1106.0      2401.0      1138.0   \n",
       "2              52.0      1467.0          190.0       496.0       177.0   \n",
       "3              52.0      1274.0          235.0       558.0       219.0   \n",
       "4              52.0      1627.0          280.0       565.0       259.0   \n",
       "\n",
       "   medianIncome  medianHouseValue  \n",
       "0        8.3252          452600.0  \n",
       "1        8.3014          358500.0  \n",
       "2        7.2574          352100.0  \n",
       "3        5.6431          341300.0  \n",
       "4        3.8462          342200.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('/Users/sandinatatu/Desktop/Tensorflow-Bootcamp-master/02-TensorFlow-Basics/cal_housing_clean.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72f6db",
   "metadata": {},
   "source": [
    "Separate your features and target data (medianHouseValue) into training and testing sets, with 33% reserved for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c39638",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :6]\n",
    "y=df['medianHouseValue']\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2e21a",
   "metadata": {},
   "source": [
    "## Scale the Feature Data\n",
    "\n",
    "Use sklearn preprocessing to create a MinMaxScaler for the feature data. Fit this scaler only to the training data. Then use it to transform X_test and X_train. Then use the scaled X_test and X_train along with pd.Dataframe to re-create two dataframes of scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bd9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ff64e",
   "metadata": {},
   "source": [
    "## Fit a Densely Connected Neural Network to the Training Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61356785",
   "metadata": {},
   "source": [
    "Construct a Densely Connected Neural Network with 3 hidden layers, each having 6 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f766548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177us/step - loss: 7355455712891240448.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13295415296.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168us/step - loss: 13235014656.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step - loss: 13251166208.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13282102272.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13236804608.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176us/step - loss: 13383601152.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13184366592.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13186378752.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13129489408.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170us/step - loss: 13422127104.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13277646848.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 13102978048.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177us/step - loss: 13299637248.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176us/step - loss: 13039740928.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step - loss: 13537258496.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13101434880.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13244439552.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step - loss: 13434278912.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13617035264.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step - loss: 13185506304.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 13157076992.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 13254739968.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182us/step - loss: 13275602944.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step - loss: 13226414080.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 13164668928.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - loss: 13398117376.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180us/step - loss: 13492171776.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step - loss: 13254041600.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176us/step - loss: 13544974336.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176us/step - loss: 13153137664.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step - loss: 13370974208.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178us/step - loss: 13587080192.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177us/step - loss: 13451708416.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - loss: 12942340096.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step - loss: 13311792128.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step - loss: 13221711872.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178us/step - loss: 13116746752.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177us/step - loss: 13259539456.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step - loss: 13157551104.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164us/step - loss: 13662935040.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169us/step - loss: 13357707264.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176us/step - loss: 13540781056.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step - loss: 13674587136.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 13519640576.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 12958720000.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 13612411904.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step - loss: 13319365632.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step - loss: 13398844416.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step - loss: 13377710080.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - loss: 13377673216.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - loss: 13319667712.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - loss: 13358081024.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178us/step - loss: 13412789248.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178us/step - loss: 13255548928.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177us/step - loss: 13524397056.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - loss: 13439824896.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - loss: 13263703040.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - loss: 13387670528.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181us/step - loss: 13341160448.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - loss: 13549797376.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - loss: 13275012096.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179us/step - loss: 13303442432.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180us/step - loss: 12942923776.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180us/step - loss: 13217414144.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180us/step - loss: 13114547200.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181us/step - loss: 13280987136.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180us/step - loss: 13285960704.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180us/step - loss: 13328155648.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178us/step - loss: 13020536832.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 13302620160.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13406605312.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 12974764032.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13313772544.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13441067008.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13623821312.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step - loss: 13081252864.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13114610688.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13638190080.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step - loss: 13367495680.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step - loss: 13383969792.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13590994944.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171us/step - loss: 13154465792.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 12960530432.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13497305088.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13654651904.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13207664640.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13515435008.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13198841856.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170us/step - loss: 13297640448.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170us/step - loss: 13477130240.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170us/step - loss: 13563857920.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13276059648.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 13190171648.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - loss: 13502567424.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13606988800.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13410825216.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13231166464.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 13234458624.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m1383/1383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - loss: 13461859328.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x174a3dc50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(6,))) \n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  \n",
    "\n",
    "\n",
    "learning_rate = 0.01  \n",
    "optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=optimizer)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=10, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d646fe",
   "metadata": {},
   "source": [
    "## Compute the RMSE on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2653ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error on Test Data: 115485.10054548162\n"
     ]
    }
   ],
   "source": [
    "mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Root Mean Squared Error on Test Data:\", mse**0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
